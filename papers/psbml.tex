% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
\usepackage{makeidx}
\usepackage{graphicx}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\begin{document}
%
\title{Parallel Machine Learning Using Spatial Model and Boosting }
%
\titlerunning{Parallel Machine Learning}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Uday Kamath\inst{1} \and Johan Kaers\inst{2} \and
Amarda Shehu\inst{1}\and Kenneth A. De Jong\inst{1}}
%
\authorrunning{Ivar Ekeland et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
\institute{
George Mason University, Fairfax VA 22003, USA,\\
\email{ukamath,ashehu,kdejong@gmu.edu} \\
Shaman Research, Heverlee 3001, Belgium,\\
\email{johankaers@telenet.be}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Recently, large scale machine learning algorithms have become increasingly important due to various factors like large dataset sizes, model complexities and the need for real time predictions. Algorithms solving these issues deal with either changing the base learning classifier to parallelize or to distribute tasks of learning parallely. Spatially structured evolutionary algorithms (SSEA) have been used for similar solutions in the evolutionary computation field. Having a structured disposition and communication based on locality results in unique dynamics that are different from a single population evolutionary algorithms, especially in finding better individuals and getting to the global optimum faster. In this paper we present a parallel spatial algorithm inspired by SSEA and boosting techniques and apply it to performing large scale machine learning. First, we present some theoretical and empirical results on a toy dataset to show a strong equivalence relationship to spatial evolutionary models. Next, experiments on various benchmark datasets are performed to show the machine learning related benefits like faster learning, improved accuracy and reducing the training dataset by removing noise.
\keywords{spatially structured evolutionary algorithms, machine learning, Boosting}
\end{abstract}
%
\section{Introduction}
\paragraph{}
The most common applications of machine learning are in supervised learning where a training set of examples (or instances) is used to learn a model with which predictions are then made on unlabeled instances in a test set. The goal of the machine learner is to generalize and learn from the training set some functions for this prediction on unseen and future data. Many applications have huge training sets that cannot even fit in the large amounts of memory typically available today. Also, during the process of learning the models, many of these algorithm perform complex operations that often need the entire dataset in memory. In some cases they scale as a polynomial factor of the training set size in space due to these complex operations involved in learning~\cite{BordesBG09}. Thus either due to the large training set size or due to the classifier model computational complexity, changes are needed to learn on large scale data. Some basic solutions like sampling the datasets can be used but introduce sampling errors and do not make optimal us of the large amount of available training data. The complex solution that many of the large scale learning algorithms employ is to change the basic classification algorithm by adding parallelization or decomposition of the tasks it is composed of.  That way, network or grid computing can both speedup  the learning process and efficiency is gained. As machine learning algorithms keep extending and more are invented, changing them to handle large scale learning becomes a non trivial problem. This raises the basic question if whether we can find a way to parallelize any basic classifier algorithm without changing it, yet gaining the advantage of faster and parallelized learning? 
\paragraph{}
Another important factor is that most of the training datasets have large imbalances i.e data of one class is a fraction of the others.There are also issues like noisy data or outliers and the presence of easier, similar or duplicate instances in the huge imbalanced data sets. These instances could be easily factored out and help reduce the data size wihtout impacting accuracy. But to factor these we still need to process the huge training data through the learning algorithm. This gives rise to another question, i.e can the large scale learner automatically remove noisy, erroneous and easy instances from the large data set while performing the large scale learning? Many ensemble based learning methods like Boosting have successfully addressed some aspect of the issue by increasing the weights or changing the sampling of instances to decrease and eliminate the noisy or easy data instances~\cite{Schapire97boosting}.
\paragraph{}
Spatially structured evolutionary algorithms (SSEA) which use topography based population distribution and network based local selection with neighbors have been very well analyzed and researched~\cite{Sarma96ananalysis}. It has been shown the ability of SSEAs to keep diverse individuals and diffusing the better individuals result in improved performance in many applications~\cite{tomassini2005spatially}.
\paragraph{}
In this paper, we combine the SSEA based approach with the boosting based approach to form a new parallel spatial boosting machine learner which we refer to as PSBML. PSBML uses the spatial network topography for parallelizing, diffusing and selecting instances from the large training set. The evaluation of instances is done on the neighborhood nodes instances and the weights are defined based on margin of difficulty in classifying the instances. It has been shown that the boosting based algorithms which increase the weights on difficulty of classifying tend to improve the performance while reducing the data set as the easy instances get eliminated and the probability of having difficult instances increases.
\paragraph{}
We invent a toy classifier which is based on an equation of circle classifying elements inside the circle as one class and outside as the other with large number of instances randomly generated. We use the data and the classifier with our algorithm for doing some basic analysis. We show on the toy classifier the theoretical and empirical equivalence of our algorithm with SSEA which helps the designer to get all the advantages of studies done in SSEA analysis. We also show the advantages of the algorithm in reducing the training data set to only the key instances that define the boundary hidden in the instances and improve the performance of the basic classifier without changing it. The advantage of the algorithm is that it can be used on any dataset like small, medium and large and can with any existing classifier without modification.To show the robustness of the algorithm we run it on various UCI benchmark data sets with varying sizes, feature sizes and characteristics. We show that independent of the datasets and the classifier, PSBML achieves in accomplishing all the tasks of improving performance from the base classifier, reducing the training data set significantly in most cases and faster execution without modifying the algorithm.

\section{Related Work}
\paragraph{}
Large scale machine learning using parallelization of algorithms like support vector machines (SVM) and decision trees have resulted in improvement in speed and accuracy~\cite{psvm}. Most of these algorithms are changed to parallelize computations like matrix transforms in SVM or tree node learning and many of them use communication infrastructure like message passing interfaces (MPI) for exchanges. MapReduce gives a generic framework for divide and conquer based approach and has been used in conjunction with learner algorithms to scale for large datasets~\cite{NIPS2006725}. Ensemble based learning on parallel networks have also been employed on various tree based algorithms for learning on enormous datasets~\cite{treeensembles,tree2}. Ensemble based methods like boosting as meta learners on weak base classifiers have showed the great potential of sampling difficult instances, reducing the datasets and improve the classification accuracy~\cite{Schapire97boosting}.
\paragraph{}
Spatially structured evolutionary algorithms have been used in various topographical configurations, communication strategies and applications showing the strengths in maintaining diversity of the population and optimally propagating best individuals in the network~\cite{tomassini2005spatially}. Analysis of various neighborhood shapes and sizes and effect of local selection has been analyzed in SSEA~\cite{Sarma96ananalysis}.

\section{Methodology}
In this section, the PSBML algorithm is described starting from a number observations about spatially structured evolutionary algorithm and boosting. First, the distribution of instances from the dataset over the spatial structure in which the base learners are embedded is covered. Every instance is assigned a weight indicating how 'difficult' it is for the learners using a measure similar to what is used in boosting. Finally, the propagation of the instances through the spatial structure is shown to result in the selection the the most highly weighted and therefore most relevant instances over time.

\subsection{Basic Spatially Structured Evolutionary Algorithms}
In spatially structured evolutionary algorithms~\cite{tomassini2005spatially}, a spatial structure is imposed on the population resulting in the appearance of distance between individuals. Close individuals form subpopulations to which mating is confined so that offspring is always placed within the vicinity of the parents. This way, evolution can explore different regions of the fitness landscape in the sub-populations. However, there is no hard boundary between these sub-populations and given enough time, individuals with a sufficiently high fitness can spread out over the entire structure. The neighborhood definition enforces this split-up into subpopulations and is important because it determines the rate of exchange of genetic material. It needs to be sufficiently slow to allow for local divergence while not hampering the flow of highly fit individuals.
\paragraph{}
In this work, the same setup is used. But instead of a population of individuals in the EA, there is a dataset of instances. And instead of searching through the fitness landscape, the training and generalization results of the learners are optimized. As in SSEA, during initialization the instances of the dataset are divided equally ovrt the nodes in the spatial structure. Here, a torroidal wraparound grid is used, imposing a 2-dimensional grid on the dataset with no boundary effects. The neighborhood which defines the nodes whose instances are involved in the reproduction steps are the similar ones from parallelized EA algorithms. e.g. C9, L5, C19, L9 as shown in Figure~\ref{Grid}

\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{./figures/Torroidal.PNG}
\caption{2D-grid and the various neighborhood C9,C19,L5 and L9}
\label{Grid}
\end{figure}


\subsection{Parallel Spatial Boosting Machine Learner (PSBML)}
Every node in the grid has a learner which uses the instances assigned to the node as its training set. The employed learners need to be able to output a confidence value for the decision they make when testing an instance. Classifiers with a fast learning algorithms like Naive Bayes and Decision Trees fall in this category.

The equivalent of the fitness function in EAs is a measure of how 'difficult' the instance is for the learners. The basic idea being that concentrating on these instances will result in smaller datasets by pruning away the redundant ones and will force the learner in the direction of the underlying decision boundary. To determine this weight for an instance, it is tested on the learners of its neighboring nodes. This results in number of classifications, each with an associated confidence $c_{ni}$. Similar to the classification margin concept used in boosting~\cite{Schapire97boosting}, the smallest confidence from any node, for any class is taken as the weight of the instance.
$$
w = \min_{i \in class}(\min_{n \in neighbor} c_{ni})
$$

This way, the weight becomes a measure of the uncertainty of the learners, allowing the algorithm to pick out the difficult instances over which the neighbors are most likely to disagree. 
\paragraph{}
Once the instance weights are calculated for all nodes, the reproduction step takes place. As in SSEAs, this happens only between the instances of a node and its neighbors. First, all these instances are collected and their weights normalized to the interval [0,1] through linear re-scaling. The instance with the lowest confidence getting weight 1, the one with the highest confidence weight 0.
$$
 w_{norm} = 1- \frac{w-w_{min}}{w_{max}-w_{min}}
$$
The instances surviving to the next epoch are picked similar to Fitness Proportion Selection in EAs~\cite{dejong01}, using roulette wheel sampling on the normalized weights.   Different from EAs, a replacement fraction parameter $p_{r}$ is used that determines the fraction of instances replaced. This is needed because in contrast to the constant fitness in GAs, the weight of an instance is not a constant since it depends on the datasets and learners in its neighbors.  Therefore, an instance needs to be evaluated multiple times so its average weight is effectively used for selection. And an easy way is to keep most instances every epoch and only replace a fraction.

\begin{itemize}
\item{} Initialization: distribute dataset over nodes
\item{} For every epoch
  \begin{itemize}
   \item{}Train all nodes
   \item{}Test all instances on their neighbor nodes
   \item{}For all nodes
   \begin{itemize}
      \item{}Collect the instances of the node and its neighbor nodes
      \item{}Normalize the weights of these instances
      \item{}For each instance, 
      \begin{itemize}
         \item{} replace it with probability $p_r$ using roulette wheel selection
         \item{} else keep it
      \end{itemize}
   \end{itemize}
   \item{} For all nodes
      \begin{itemize}
         \item{} Replace the instances with the selected ones
      \end{itemize}
  \end{itemize}
\end{itemize}


\subsection{Growth Curve Analysis of Selection}
In growth and diffusion studies and its application in spatial evolutionary learning, the migration of individuals through the nodes has been studied under various selection methods~\cite{Sarma96ananalysis,Banks}. For spatially structured populations using fitness proportionate selection, the growth rate of the fittest individual in the population is described by a logisitc curve. 
$$
p_{b,t} = \frac{1}{1+(\frac{1}{p_{b,0}}-1)e^{-at}}
$$
with \begin{itemize}
\item{}  $p_{b,t} = $ the proportion of the fittest individual at time $t$
\item{}  $p_{b,0}$  = the proportion of the fittest individual at time $0$
\item{} $a$ = the growth coefficient which depends on shape and size of the neighborhood relative to the entire grid.
\end{itemize}

\paragraph{}
Again, a close similarity can be made between these analysis and this work. The fittest individual becomes the instance with the highest weight and the fitness proportionate selection is essentially the same as the roulette wheel sampling on the normalized instance weights used here. This equivalence means that the propagation of difficult instances through the grid follows the same dynamics described by the logistic curve equation above. Also, because there is only instance selection, no mutations or modifications are made to the instances and the highly weighted, difficult instances will gradually replace the easy ones in more and more nodes. The total set of unique instances taken over all nodes will therefore concentrate on the fewer but more relevant ones for the learning problem. As such, this dataset pruning result is already highly useful in practice because it can be used as a pre-processor on the complete dataset before running a more advanced but slower learner on the pruned data-set.

\paragraph{}
The figure below illustrates how the instance weights propagate through a grid of nodes setup with different neighborhood shapes. It uses an idealized configuration where a 32x32 grid contains a single instance of per node. Initially all instances are set to weight 0, except a single one, which is set to 2.0. The graph plots the fraction of instances with weight 2.0 over 50 epochs for the various neighborhoods. Again, instances are selected using weight proportionate selection. This corresponds to $P_{b,t}$  in the above equation. As described in~\cite{Sarma96ananalysis}, the neighborhood size influences the growth coefficient $a$. The effect of the neighborhood radii for the neighborhoods C13 – C9 - L9 –  L5 – L4 is clearly visible in the successive logistic curves from left to right as shown in Figure ~\ref{WeightChanges}

\begin{figure}
\centering
 \includegraphics[width=0.6\columnwidth]{./figures/WeightChanges.PNG}
\caption{Fraction of instances with weight = 2. Neighborhoods C13, C9, L9, L5, L4}
\label{WeightChanges}
\end{figure}

\section{Experiments}
\subsection{Experiment 1: A Toy Circle Classifier}
A toy problem using a circle-fitting learner is used to illustrate the predicted behavior in a simple configuration. 

\subsubsection{Circle Classifier Experiment}
The nodes form a 5x5 wraparound grid over which a dataset of 10000 instances is equally divided. The instances are drawn at random from the unit square, with the instances closer than 0.4 to the origin classified as - and the others as +. The learner fits a circle to its training set by fixing its radius to the average distance from the origin of the smallest positive and the largest negative instance. For instance testing, the learner outputs - when the instance falls within its circle, + otherwise. The confidence is the distance of the instance from the circle edge. The neighborhood is the C9 one, with each node having the 8 neighbors closest to it in grid-distance. 

\begin{figure}
    \centering
   \begin{tabular}{cc}
   \includegraphics[width=0.45\columnwidth]{./figures/CircleData.PNG} &\includegraphics[width=0.45\columnwidth] {./figures/BeforeAfter.PNG}  
    \end{tabular}
    \caption{Circle InitialTotal Data and Node Distribution Before and After runs}
    \label{Circle} 
\end{figure}


Running the PSBML algorithm on this setup for 100 epochs, we determine the size of set of distinct instances over all nodes and measure the average distance of the instances from the origin.  

\subsubsection{Classification Margin Observations}
Initially, the average radius of the instances is 0.797884 because they are distributed equally over the unit square (TODO: formula, explain why). But soon, the highly weighted, difficult instances close to the edge of the circle dominate the dataset, moving the average radius towards the 0.4 radius of the circle hidden in the dataset.

\begin{figure}
    \centering
   \begin{tabular}{cc}
   \includegraphics[width=0.45\columnwidth]{./figures/RadiusDecrease.PNG} &\includegraphics[width=0.45\columnwidth] {./figures/PruneSize.PNG}  
    \end{tabular}
    \caption{Circle Radius Change and Distinct Instance sizes}
    \label{RadiusChangeAndDistinct} 
\end{figure}
\subsubsection{Data Set Reduction Observation}
The size of the set of distinct instances keeps decreasing even after the average radius has converged to 0.4. This shows the most difficult instances continue to be selected the next epoch, eventually focussing on the few + and - instances most close to the circle edge.

\subsubsection{Growth Curve Observations}
For a random node, if we plot the mean and median weights over all the generations, we can see a logistic curve fitting for the node and its neighbors as shown in Figure ~\ref{MeanAndMedianWeight} confirming the equivalence to the growth curve analyses of SSEAs.

\begin{figure}
    \centering
   \begin{tabular}{cc}
   \includegraphics[width=0.45\columnwidth]{./figures/MeanWeight.PNG} &\includegraphics[width=0.45\columnwidth] {./figures/MedianWeight.PNG}  
    \end{tabular}
    \caption{Circle Radius Change and Distinct Instance sizes}
    \label{MeanAndMedianWeight} 
\end{figure}



\subsection{Experiment 2: Benchmark Datasets}

\subsubsection{UCI Benchmark datasets Experiment}
\begin{enumerate}
\item{Impact of Neighborhood Size and Shape on Learning}\\
UCI Chess (King-Rock v/s King Pawn) dataset with 3196 instances, 36 attributes and 2 classes is run against various neighborhood grid size and shape as shown in the Figure ~\ref{Chess}. We used 5X5 grid with Naive Bayes Classifier with discretization for numeric features.It clearly shows that with training as we remove noisy data, the error rate decreases. The error rate increases after a while when more data are pruned.C13 shows better decrease in error rate as compared to C9, and similarly L9 shows better curve than L5. The average reduction in training data set is almost similar for all the shapes. 

\begin{figure}
    \centering
   \begin{tabular}{cc}
   \includegraphics[width=0.45\columnwidth]{./figures/ChessErrorRate.PNG} &\includegraphics[width=0.45\columnwidth] {./figures/ChessDataSize.PNG}  
    \end{tabular}
    \caption{Circle Radius Change and Distinct Instance sizes}
    \label{Chess} 
\end{figure}

\item{Experimental Analyses on Benchmark Datasets}
Next, 9 Datasets carefully chosen from UCI machine learning repository~\cite{UCI} with medium to large datasets, with different statistical properties, different attributes/features and different number of classes to test the robustness of the PSBML algorithm. Each data set has training and testing set, if no separate test set is available we split it 90-10 and use 10 \% for testing. We use 5x5 grid with C9 configuration and replacement fraction of 0.2. We tested with 3 base classifiers Naïve Bayes with discretization, Decision Tree (J48) and SVM (SMO) from Weka~\cite{hall09}. Each of the experiments on PSBML are done 30 times and mean is reported in the table below. The standard deviation for most PSBML runs was below 0.1 and for space considerations we have not included below.Also in the table we have used abbr for PSBML with Naive Bayes as P-NB, PSBML with Decision Tree as P-DT and PSBML with SVM as P-SVM. Similarly \#F is for number of features and \#C for number of classes in the dataset.Along with changes to accuracy we also show training set reduction acheived finally with prefix \# for each algorithm.

\begin{table}
\caption{UCI Benchmark dataset comparisons}
\centering
\begin{tabular}{@{}llllllllllllll@{}}
\hline
Dataset &Train&Test&\#F&\#C&NB&P-NB&\#P-NB&DT&P-DT&\#P-DT&SVM&P-SVM&\#P-SVM\\\hline\hline
Chess&3196&3196&36&2&88.32&\bf93&191&99.65&99.64&2678&96.24&\bf97.1&2001\\
SpamBase&4600&4600&57&2&79.52&\bf94&752&97.1745&96.1&2667&90.76&90.78&3078\\
Digit&10992&10992&256&10&84.409&\bf90&484&79.51&\bf80.12&302&87.97&\bf88.45&1297\\
Magic&19020&1902&10&2&78.21&\bf83.1&4544&85.49&86.1&7699&79.33&80.12&3715\\
Adult&32560&16279&14&2&83.19&\bf89.01&5625&85.83&85.61&9163&85.26&\bf86.1&23409\\
W8A&49749&14951&300&2&96.7&\bf98.1&7234&NA&NA&NA&NA&NA&NA\\
Cod-RNA&271617 &59535&8&2&78.11&\bf90.01&9157&95.12&96.12&145001&93.9&84.1&147234\\
CoverType&581012&58102&54&7&79.15&\bf85.1&&NA&NA&NA&NA&NA&NA\\
KDDCup99&4000000&311000&42&24&98.89&\bf99.65&45034&NA&NA&NA&NA&NA&NA\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item PSBML with naïve bayes in all the datasets have statistically significant with 95\% confidence and using t-tests,show  improvement performance in accuracy while reducing the training data set and speeding the performance by a magnitude.
\item PSBML with decision trees and PSBML with SVM also show improvements but some of them are not statistically significant as naïve bayes.
\item Some of the results with PSBML are the best ever recorded like PSBML-NB for Adult, digits etc showing the impact of meta learning provided by the classifier.

\end{itemize}



\end{enumerate}


\section{Conclusion and Future work}

\section{Implementation and Code Availability}

\bibliographystyle{plain}
\bibliography{MLPaper-2012}  
\end{document}
